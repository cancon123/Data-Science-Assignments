{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks for Classification\n",
    "Here we will provide a code snippet that classifies a dataset of handwritten digits. The layout of the code will provide you with insights and by adapting it, you will solve a classification problem in the next exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we just create a directory to store the dataset in\n",
    "!mkdir mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will download the dataset and put it into datastructures provided by Pytorch. The transforms used here are specific to the dataset and transform the images into a `torch.tensor`, which is the datastructure used by pytorch (for an in depth tutorial on how Pytorch works and can be used for custom problems you can visit the [tutorials](https://pytorch.org/tutorials/)). In a second step the data points are normalized, which is a usual preprocessing step when training neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.3%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./mnist/MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist/MNIST\\raw\\train-images-idx3-ubyte.gz to ./mnist/MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./mnist/MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist/MNIST\\raw\\train-labels-idx1-ubyte.gz to ./mnist/MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./mnist/MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-dae6eba9b9b1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mtransforms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.1307\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0.3081\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     ])\n\u001b[1;32m----> 5\u001b[1;33m dataset_train = datasets.MNIST('./mnist/', train=True, download=True,\n\u001b[0m\u001b[0;32m      6\u001b[0m                    transform=transform)\n\u001b[0;32m      7\u001b[0m dataset_test = datasets.MNIST('./mnist/', train=False,\n",
      "\u001b[1;32mc:\\users\\ardad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torchvision\\datasets\\mnist.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, root, train, transform, target_transform, download)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdownload\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_exists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ardad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torchvision\\datasets\\mnist.py\u001b[0m in \u001b[0;36mdownload\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    185\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Downloading {url}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m                     \u001b[0mdownload_and_extract_archive\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdownload_root\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw_folder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmd5\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmd5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mURLError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Failed to download (trying next):\\n{error}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ardad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torchvision\\datasets\\utils.py\u001b[0m in \u001b[0;36mdownload_and_extract_archive\u001b[1;34m(url, download_root, extract_root, filename, md5, remove_finished)\u001b[0m\n\u001b[0;32m    432\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    433\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 434\u001b[1;33m     \u001b[0mdownload_url\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdownload_root\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmd5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    435\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    436\u001b[0m     \u001b[0marchive\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdownload_root\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ardad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torchvision\\datasets\\utils.py\u001b[0m in \u001b[0;36mdownload_url\u001b[1;34m(url, root, filename, md5, max_redirect_hops)\u001b[0m\n\u001b[0;32m    142\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Downloading \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0murl\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" to \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m             \u001b[0m_urlretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mURLError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# type: ignore[attr-defined]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"https\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ardad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torchvision\\datasets\\utils.py\u001b[0m in \u001b[0;36m_urlretrieve\u001b[1;34m(url, filename, chunk_size)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_urlretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1024\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"User-Agent\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUSER_AGENT\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m         \u001b[0m_save_response_content\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mb\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ardad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torchvision\\datasets\\utils.py\u001b[0m in \u001b[0;36m_save_response_content\u001b[1;34m(content, destination, length)\u001b[0m\n\u001b[0;32m     35\u001b[0m ) -> None:\n\u001b[0;32m     36\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdestination\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"wb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpbar\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcontent\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m             \u001b[1;31m# filter out keep-alive new chunks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mchunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ardad\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torchvision\\datasets\\utils.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_urlretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1024\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"User-Agent\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUSER_AGENT\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m         \u001b[0m_save_response_content\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mb\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ardad\\appdata\\local\\programs\\python\\python39\\lib\\http\\client.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    456\u001b[0m             \u001b[1;31m# Amount is given, implement using readinto\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    457\u001b[0m             \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 458\u001b[1;33m             \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    459\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    460\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ardad\\appdata\\local\\programs\\python\\python39\\lib\\http\\client.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    500\u001b[0m         \u001b[1;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m         \u001b[1;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 502\u001b[1;33m         \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    503\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    504\u001b[0m             \u001b[1;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ardad\\appdata\\local\\programs\\python\\python39\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    702\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    703\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 704\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    705\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    706\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "dataset_train = datasets.MNIST('./mnist/', train=True, download=True,\n",
    "                   transform=transform)\n",
    "dataset_test = datasets.MNIST('./mnist/', train=False,\n",
    "                   transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(dataset_train, batch_size=16)\n",
    "test_loader = torch.utils.data.DataLoader(dataset_test, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can have a look at the dataset to get a feeling on how it looks. The goal of the neural network is to use the image as an input and predict the number the image shows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,2),dpi=100)\n",
    "for k,i in enumerate(np.random.randint(0, len(dataset_train),5)):\n",
    "    plt.subplot(1,5,k+1)\n",
    "    img, label = train_loader.dataset[i]\n",
    "    img = img[0].numpy()\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.title('Label = '+str(label))\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this prediction we will use a simple feedforward neural network architecture with one hidden layer and  ReLU-activation function. This activation function has excellent properties for training neural networks, since they solve the vanishing gradient problem and allow for deeper neural networks, but there are many different activation functions that can be used (see the [documentation](https://pytorch.org/docs/stable/nn.html) for a full list).\n",
    "$$\\mathrm{ReLU}(x) = max(0,x)$$\n",
    "As output activation we use a logarithmic softmax, which can be seen as a mapping to the likelihoods of an image belonging to a certain class.\n",
    "\n",
    "The architecture of our neural network consists of two fully connected layers defined in `__init__(self)`, which first map the $28*28=784$ pixels to 128 intermidiate values via matrix multiplication and an added bias term. This intermidiate vector is then mapped to a 10-dimensional ouput vector. The `forward(self, x)` method defines the forward pass of the neural network, where first the image is flattened into a vector, then the first fully connected layer is applied, then a ReLU activation, then the second fully connected layer and finally the logarithmic softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 128) #The first parameter (aka in_parameters) means that the no. of parameters the current linear layers expects from the previous layer.\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining our architecture, we need to perform the training of the parameters. This optimization procedure is in most instances based on a gradient descent variety. Since we have a lot of training samples here, computing the full gradient of the loss function with respect to each adaptable weight is very expensive. Instead we opt for an approximation of the gradient of a minibatch of samples (e.g. 16 samples). This gives an approximate gradient direction, which allows us to update our weights and minimize our loss function more efficiently. Below you find a loop over our training and testing dataset. Read through the code and try to understand what each statement means and what happens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the loop below and train the model (this might take a while)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of iterations\n",
    "epochs = 10\n",
    "# step width in gradient-based optimization\n",
    "learning_rate = 0.0001\n",
    "# initialize our model with random parameters\n",
    "model = Net()\n",
    "# choose an optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "# choose a loss function\n",
    "loss_function = F.nll_loss\n",
    "\n",
    "# initialize lists for plotting\n",
    "learning_curve_train = []\n",
    "learning_curve_test = []\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    \n",
    "    # temporary variables to track model convergence\n",
    "    loss_train = []\n",
    "    loss_test = []\n",
    "    correct = 0\n",
    "    \n",
    "    # Training Loop\n",
    "    for data, target in train_loader:\n",
    "        # set gradient to zero\n",
    "        optimizer.zero_grad()\n",
    "        # predict with current model\n",
    "        output = model(data)\n",
    "        # compute loss of predictions and target values\n",
    "        loss = loss_function(output, target)\n",
    "        # Backpropagation of errors gives gradients\n",
    "        loss.backward()\n",
    "        # Update weights with approximate gradient\n",
    "        optimizer.step()\n",
    "        # Store loss\n",
    "        loss_train.append(loss.item())\n",
    "\n",
    "    # Stop tracking gradients for evaluation phase\n",
    "    with torch.no_grad():\n",
    "        # Test Loop\n",
    "        for data, target in test_loader:\n",
    "            # predict with current model\n",
    "            output = model(data)\n",
    "            # compute loss\n",
    "            loss = loss_function(output, target)\n",
    "            # store loss\n",
    "            loss_test.append(loss.item())\n",
    "            # transform output into predicted class\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            # compare predictions and labels and store \n",
    "            # number of correct predictions\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    # Store loss at each iteration for plotting\n",
    "    learning_curve_train.append(np.mean(loss_train))\n",
    "    learning_curve_test.append(np.mean(loss_test))\n",
    "    \n",
    "    # Print to Stdout\n",
    "    print('Epoch: {}\\tTrain loss: {:.4f}\\tTest loss: {:.4f}\\tTest Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "        epoch, learning_curve_train[-1], learning_curve_test[-1], \n",
    "        correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check if the model generalizes by looking at the learning curve (loss of training and testing data at each iteration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(1, epochs+1), learning_curve_train, label='Train')\n",
    "plt.plot(np.arange(1, epochs+1), learning_curve_test, label='Test')\n",
    "plt.xlabel('# Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can look at some of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,2),dpi=100)\n",
    "for k,i in enumerate(np.random.randint(0, len(dataset_test),5)):\n",
    "    plt.subplot(1,5,k+1)\n",
    "    img, label = test_loader.dataset[i]\n",
    "    pred = torch.argmax(model(img)).item()\n",
    "    img = img[0].numpy()\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.title('Prediction = '+str(pred))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.xlabel('True = '+str(label))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of Stars (4P)\n",
    "In this exercise we are going to adapt the previously provided code to work on a comparatively small dataset of star measurements from [Kaggle](https://www.kaggle.com/deepu1109/star-dataset). From just four measurements (Temperature, Luminosity, Radius, Absolute magnitude), we will try to predict the star type (Browm Dwarf, Red Dwarf, White Dwarf, Main Sequence, Supergiant, Hypergiant) by using a Feedforward Neural Network architecture that was trained on 200 measurements. By determining hyperparameters we will optimize our model before we will test the generalization of the model to not seen instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First load the dataset in `star_data.csv` from Moodle with pandas and familiarize yourself with the dataset. Extract the data into Numpy-arrays called `x` and `y` for input (the four measurments) and output (the star type) data respectively and make sure that the each array has the correct datatype (`np.float32` for `x` and `np.longlong` for `y`. **Hint**: you can use `.to_numpy()` on pandas dataframes and `.astype()` on numpy arrays to change the datatype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Temperature (K)',\n",
       " 'Luminosity(L/Lo)',\n",
       " 'Radius(R/Ro)',\n",
       " 'Absolute magnitude(Mv)',\n",
       " 'Star type',\n",
       " 'Star color',\n",
       " 'Spectral Class']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"star_data.csv\")\n",
    "header = list(df.columns)\n",
    "header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[header[4]].to_numpy(dtype=np.longlong)\n",
    "x = df[df.columns[0:4]].to_numpy()\n",
    "#x[:,1]  #getting columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function `rescale(x)` that min-max-scales each of your input-values to values between 0 and 1 (see [here](https://towardsdatascience.com/everything-you-need-to-know-about-min-max-normalization-in-python-b79592732b79) for an introduction). \n",
    "$$x_{i,rescaled}=\\frac{x_i-x_{i,min}}{x_{i,max}-x_{i,min}}$$\n",
    "Split the pairs of inputs and outputs into 200 training samples and 40 testing samples. Shuffle your data beforehand to prevent unexpected behaviour. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0024"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(x[:,0])  #40000\n",
    "min(x[:,0])  #1939\n",
    "x[:,1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale(v,dataset):\n",
    "    x_rescaled = (v - min(dataset)) / (max(dataset)-min(dataset))\n",
    "    return x_rescaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rescale(x[:,2][0],x[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized = []\n",
    "for i in range(4):\n",
    "    for v in x[:,i]:\n",
    "        normalized.append(rescale(v,x[:,i])) \n",
    "        \n",
    "arr = np.array(normalized)\n",
    "normalized_x = np.array(np.split(arr,4))\n",
    "#normalized_x[0]\n",
    "normalized_x_reshaped = normalized_x.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use train_test_split to split the data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(normalized_x_reshaped, y, test_size=40, train_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#idx = np.arange(len(normalized_x[0]))\n",
    "#np.random.shuffle(indices)\n",
    "#xx_train, yy_train = x[idx[:200]], y[idx[:200]]\n",
    "#xx_test, yy_test = x[idx[200:]], y[idx[200:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the introductory example on the MNIST dataset, the dataset was already provided in a clean datastructure which allowed for convenient usage. Now we have to create our own dataset. For this we will use the `Dataset` class provided in Pytorch. Complete the `__len__(self)` and `__getitem__(self, idx)` methods to return the number of samples in the dataset and the sample (input and label) with index `idx`. Print the number of training- and test-datasets and output some of the input and output pairs by calling 5 random indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 40\n",
      "(tensor([[1.6581e-01, 1.0890e-05, 3.0810e-04, 3.3202e-01]]), tensor(3))\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class StarDataset(Dataset):\n",
    "    def __init__(self, x, y,dtype=torch.float32):\n",
    "        self.x = torch.from_numpy(x).to(dtype)\n",
    "        self.y = torch.from_numpy(y).to(dtype)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.y.shape[0]  # return number of samples in dataset\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx].squeeze().long()  #return sample (input and label) with index\n",
    "                                                          #.squeeze().long(), we put it in order to not have an array in y_output. (e.g. tensor(2))  \n",
    "#StarDataset(X_train,Y_train).__getitem__(123)   #index can be up to 200.\n",
    "dataset_train = StarDataset(X_train,Y_train,dtype=torch.float32)\n",
    "dataset_test = StarDataset(X_test,Y_test,dtype=torch.float32)\n",
    "print(len(dataset_train),len(dataset_test))\n",
    "random_indices = np.random.randint(len(dataset_test), size=1)\n",
    "print(dataset_train[random_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `DataLoader` class, create a dataloader of your training- and test-data. You can specify a batch size for loading the data. This batch size can be seen as a hyperparameter, which allows you to tune the convergence of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(dataset_train, batch_size=64) #The batch size is usually set between 64 and 256. The batch size does have an effect on the final test accuracy. One way to think about it is that smaller batches means that the number of parameter updates per epoch is greater\n",
    "test_dataloader = DataLoader(dataset_test, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapt the previously provided `Net` class to work with the new data. For this change the architecture to reflect the new input and output dimensions of your data. You can also add additional layers and change the sizes of the hidden layers to tune model performance. **Hint:** Remove the `flatten()` function, since our data is already a vector and not a matrix. You can change the width of your hidden layers by changing the `out_features`-parameter in a `nn.Linear`-layer and the `in_features` of the following `nn.Linear`-layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()   #inherit from nn.Module class\n",
    "        self.fc1 = nn.Linear(4, 100) #The first parameter (aka in_parameters) means that the no. of parameters the current linear layers expects from the previous layer.\n",
    "        self.fc2 = nn.Linear(100, 10)\n",
    "        self.fc3 = nn.Linear(10, 6) #We have 6 possible solutions (aka star types) !!!\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the provide code of the training and evaluation loop to the train your new model on the star data. Tune the hyperparameters (i.e., number of hidden layers, width of hidden layers, used activation functions, learning rate, number of epochs, training batch size) to optimize the training procedure and model performance and describe the influence of the different hyperparameters on the model performance and the training procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\tTrain loss: 1.7946\tTest loss: 1.7954\tTest Accuracy: 9/40 (22%)\n",
      "Epoch: 2\tTrain loss: 1.7487\tTest loss: 1.7685\tTest Accuracy: 9/40 (22%)\n",
      "Epoch: 3\tTrain loss: 1.7144\tTest loss: 1.7296\tTest Accuracy: 10/40 (25%)\n",
      "Epoch: 4\tTrain loss: 1.6682\tTest loss: 1.6774\tTest Accuracy: 12/40 (30%)\n",
      "Epoch: 5\tTrain loss: 1.6093\tTest loss: 1.6065\tTest Accuracy: 14/40 (35%)\n",
      "Epoch: 6\tTrain loss: 1.5386\tTest loss: 1.5266\tTest Accuracy: 14/40 (35%)\n",
      "Epoch: 7\tTrain loss: 1.4629\tTest loss: 1.4503\tTest Accuracy: 14/40 (35%)\n",
      "Epoch: 8\tTrain loss: 1.3882\tTest loss: 1.3845\tTest Accuracy: 14/40 (35%)\n",
      "Epoch: 9\tTrain loss: 1.3203\tTest loss: 1.3271\tTest Accuracy: 14/40 (35%)\n",
      "Epoch: 10\tTrain loss: 1.2464\tTest loss: 1.2773\tTest Accuracy: 17/40 (42%)\n",
      "Epoch: 11\tTrain loss: 1.1853\tTest loss: 1.2142\tTest Accuracy: 17/40 (42%)\n",
      "Epoch: 12\tTrain loss: 1.1032\tTest loss: 1.1759\tTest Accuracy: 18/40 (45%)\n",
      "Epoch: 13\tTrain loss: 1.0360\tTest loss: 1.1261\tTest Accuracy: 19/40 (48%)\n",
      "Epoch: 14\tTrain loss: 0.9739\tTest loss: 1.0626\tTest Accuracy: 19/40 (48%)\n",
      "Epoch: 15\tTrain loss: 0.9157\tTest loss: 0.9905\tTest Accuracy: 19/40 (48%)\n",
      "Epoch: 16\tTrain loss: 0.8553\tTest loss: 0.9265\tTest Accuracy: 19/40 (48%)\n",
      "Epoch: 17\tTrain loss: 0.7985\tTest loss: 0.8761\tTest Accuracy: 22/40 (55%)\n",
      "Epoch: 18\tTrain loss: 0.7560\tTest loss: 0.8455\tTest Accuracy: 23/40 (58%)\n",
      "Epoch: 19\tTrain loss: 0.7274\tTest loss: 0.8195\tTest Accuracy: 23/40 (58%)\n",
      "Epoch: 20\tTrain loss: 0.7019\tTest loss: 0.7993\tTest Accuracy: 24/40 (60%)\n",
      "Epoch: 21\tTrain loss: 0.6789\tTest loss: 0.7737\tTest Accuracy: 24/40 (60%)\n",
      "Epoch: 22\tTrain loss: 0.6579\tTest loss: 0.7571\tTest Accuracy: 24/40 (60%)\n",
      "Epoch: 23\tTrain loss: 0.6417\tTest loss: 0.7406\tTest Accuracy: 25/40 (62%)\n",
      "Epoch: 24\tTrain loss: 0.6283\tTest loss: 0.7206\tTest Accuracy: 25/40 (62%)\n",
      "Epoch: 25\tTrain loss: 0.6119\tTest loss: 0.6937\tTest Accuracy: 26/40 (65%)\n",
      "Epoch: 26\tTrain loss: 0.5966\tTest loss: 0.6681\tTest Accuracy: 28/40 (70%)\n",
      "Epoch: 27\tTrain loss: 0.5765\tTest loss: 0.6460\tTest Accuracy: 30/40 (75%)\n",
      "Epoch: 28\tTrain loss: 0.5586\tTest loss: 0.6229\tTest Accuracy: 30/40 (75%)\n",
      "Epoch: 29\tTrain loss: 0.5365\tTest loss: 0.5994\tTest Accuracy: 30/40 (75%)\n",
      "Epoch: 30\tTrain loss: 0.5156\tTest loss: 0.5774\tTest Accuracy: 29/40 (72%)\n",
      "Epoch: 31\tTrain loss: 0.4955\tTest loss: 0.5560\tTest Accuracy: 29/40 (72%)\n",
      "Epoch: 32\tTrain loss: 0.4764\tTest loss: 0.5427\tTest Accuracy: 30/40 (75%)\n",
      "Epoch: 33\tTrain loss: 0.4567\tTest loss: 0.5354\tTest Accuracy: 30/40 (75%)\n",
      "Epoch: 34\tTrain loss: 0.4415\tTest loss: 0.5261\tTest Accuracy: 30/40 (75%)\n",
      "Epoch: 35\tTrain loss: 0.4287\tTest loss: 0.5153\tTest Accuracy: 30/40 (75%)\n",
      "Epoch: 36\tTrain loss: 0.4175\tTest loss: 0.5128\tTest Accuracy: 29/40 (72%)\n",
      "Epoch: 37\tTrain loss: 0.4062\tTest loss: 0.5098\tTest Accuracy: 29/40 (72%)\n",
      "Epoch: 38\tTrain loss: 0.3983\tTest loss: 0.5013\tTest Accuracy: 29/40 (72%)\n",
      "Epoch: 39\tTrain loss: 0.3910\tTest loss: 0.4948\tTest Accuracy: 29/40 (72%)\n",
      "Epoch: 40\tTrain loss: 0.3854\tTest loss: 0.4944\tTest Accuracy: 29/40 (72%)\n",
      "Epoch: 41\tTrain loss: 0.3790\tTest loss: 0.4897\tTest Accuracy: 30/40 (75%)\n",
      "Epoch: 42\tTrain loss: 0.3771\tTest loss: 0.4892\tTest Accuracy: 30/40 (75%)\n",
      "Epoch: 43\tTrain loss: 0.3703\tTest loss: 0.4781\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 44\tTrain loss: 0.3709\tTest loss: 0.4825\tTest Accuracy: 30/40 (75%)\n",
      "Epoch: 45\tTrain loss: 0.3647\tTest loss: 0.4725\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 46\tTrain loss: 0.3704\tTest loss: 0.4851\tTest Accuracy: 30/40 (75%)\n",
      "Epoch: 47\tTrain loss: 0.3612\tTest loss: 0.4720\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 48\tTrain loss: 0.3679\tTest loss: 0.4606\tTest Accuracy: 30/40 (75%)\n",
      "Epoch: 49\tTrain loss: 0.3524\tTest loss: 0.4785\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 50\tTrain loss: 0.3629\tTest loss: 0.4824\tTest Accuracy: 30/40 (75%)\n",
      "Epoch: 51\tTrain loss: 0.3550\tTest loss: 0.4588\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 52\tTrain loss: 0.3587\tTest loss: 0.4541\tTest Accuracy: 30/40 (75%)\n",
      "Epoch: 53\tTrain loss: 0.3429\tTest loss: 0.4649\tTest Accuracy: 30/40 (75%)\n",
      "Epoch: 54\tTrain loss: 0.3427\tTest loss: 0.4637\tTest Accuracy: 30/40 (75%)\n",
      "Epoch: 55\tTrain loss: 0.3408\tTest loss: 0.4596\tTest Accuracy: 30/40 (75%)\n",
      "Epoch: 56\tTrain loss: 0.3389\tTest loss: 0.4529\tTest Accuracy: 30/40 (75%)\n",
      "Epoch: 57\tTrain loss: 0.3393\tTest loss: 0.4454\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 58\tTrain loss: 0.3404\tTest loss: 0.4666\tTest Accuracy: 30/40 (75%)\n",
      "Epoch: 59\tTrain loss: 0.3391\tTest loss: 0.4484\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 60\tTrain loss: 0.3412\tTest loss: 0.4433\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 61\tTrain loss: 0.3320\tTest loss: 0.4621\tTest Accuracy: 30/40 (75%)\n",
      "Epoch: 62\tTrain loss: 0.3337\tTest loss: 0.4440\tTest Accuracy: 30/40 (75%)\n",
      "Epoch: 63\tTrain loss: 0.3330\tTest loss: 0.4420\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 64\tTrain loss: 0.3277\tTest loss: 0.4563\tTest Accuracy: 29/40 (72%)\n",
      "Epoch: 65\tTrain loss: 0.3281\tTest loss: 0.4441\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 66\tTrain loss: 0.3298\tTest loss: 0.4376\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 67\tTrain loss: 0.3260\tTest loss: 0.4496\tTest Accuracy: 30/40 (75%)\n",
      "Epoch: 68\tTrain loss: 0.3246\tTest loss: 0.4377\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 69\tTrain loss: 0.3269\tTest loss: 0.4347\tTest Accuracy: 30/40 (75%)\n",
      "Epoch: 70\tTrain loss: 0.3236\tTest loss: 0.4486\tTest Accuracy: 29/40 (72%)\n",
      "Epoch: 71\tTrain loss: 0.3230\tTest loss: 0.4265\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 72\tTrain loss: 0.3256\tTest loss: 0.4323\tTest Accuracy: 30/40 (75%)\n",
      "Epoch: 73\tTrain loss: 0.3201\tTest loss: 0.4521\tTest Accuracy: 29/40 (72%)\n",
      "Epoch: 74\tTrain loss: 0.3225\tTest loss: 0.4335\tTest Accuracy: 30/40 (75%)\n",
      "Epoch: 75\tTrain loss: 0.3219\tTest loss: 0.4271\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 76\tTrain loss: 0.3193\tTest loss: 0.4499\tTest Accuracy: 29/40 (72%)\n",
      "Epoch: 77\tTrain loss: 0.3208\tTest loss: 0.4286\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 78\tTrain loss: 0.3258\tTest loss: 0.4351\tTest Accuracy: 29/40 (72%)\n",
      "Epoch: 79\tTrain loss: 0.3171\tTest loss: 0.4412\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 80\tTrain loss: 0.3242\tTest loss: 0.4444\tTest Accuracy: 30/40 (75%)\n",
      "Epoch: 81\tTrain loss: 0.3203\tTest loss: 0.4126\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 82\tTrain loss: 0.3294\tTest loss: 0.4314\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 83\tTrain loss: 0.3157\tTest loss: 0.4446\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 84\tTrain loss: 0.3276\tTest loss: 0.4471\tTest Accuracy: 30/40 (75%)\n",
      "Epoch: 85\tTrain loss: 0.3238\tTest loss: 0.4127\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 86\tTrain loss: 0.3303\tTest loss: 0.4252\tTest Accuracy: 30/40 (75%)\n",
      "Epoch: 87\tTrain loss: 0.3147\tTest loss: 0.4378\tTest Accuracy: 30/40 (75%)\n",
      "Epoch: 88\tTrain loss: 0.3248\tTest loss: 0.4416\tTest Accuracy: 30/40 (75%)\n",
      "Epoch: 89\tTrain loss: 0.3206\tTest loss: 0.4143\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 90\tTrain loss: 0.3222\tTest loss: 0.4191\tTest Accuracy: 30/40 (75%)\n",
      "Epoch: 91\tTrain loss: 0.3093\tTest loss: 0.4362\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 92\tTrain loss: 0.3165\tTest loss: 0.4346\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 93\tTrain loss: 0.3144\tTest loss: 0.4127\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 94\tTrain loss: 0.3136\tTest loss: 0.4166\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 95\tTrain loss: 0.3061\tTest loss: 0.4363\tTest Accuracy: 30/40 (75%)\n",
      "Epoch: 96\tTrain loss: 0.3109\tTest loss: 0.4230\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 97\tTrain loss: 0.3077\tTest loss: 0.4114\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 98\tTrain loss: 0.3045\tTest loss: 0.4143\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 99\tTrain loss: 0.3083\tTest loss: 0.4367\tTest Accuracy: 30/40 (75%)\n",
      "Epoch: 100\tTrain loss: 0.3126\tTest loss: 0.4078\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 101\tTrain loss: 0.3203\tTest loss: 0.4168\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 102\tTrain loss: 0.3100\tTest loss: 0.4277\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 103\tTrain loss: 0.3244\tTest loss: 0.4368\tTest Accuracy: 30/40 (75%)\n",
      "Epoch: 104\tTrain loss: 0.3241\tTest loss: 0.4056\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 105\tTrain loss: 0.3264\tTest loss: 0.4060\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 106\tTrain loss: 0.3046\tTest loss: 0.4279\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 107\tTrain loss: 0.3113\tTest loss: 0.4232\tTest Accuracy: 30/40 (75%)\n",
      "Epoch: 108\tTrain loss: 0.3064\tTest loss: 0.4118\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 109\tTrain loss: 0.3014\tTest loss: 0.4019\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 110\tTrain loss: 0.2986\tTest loss: 0.4159\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 111\tTrain loss: 0.2998\tTest loss: 0.4076\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 112\tTrain loss: 0.3005\tTest loss: 0.4072\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 113\tTrain loss: 0.3003\tTest loss: 0.4045\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 114\tTrain loss: 0.3033\tTest loss: 0.4137\tTest Accuracy: 30/40 (75%)\n",
      "Epoch: 115\tTrain loss: 0.3026\tTest loss: 0.4121\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 116\tTrain loss: 0.3084\tTest loss: 0.4155\tTest Accuracy: 30/40 (75%)\n",
      "Epoch: 117\tTrain loss: 0.3042\tTest loss: 0.3973\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 118\tTrain loss: 0.3184\tTest loss: 0.4159\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 119\tTrain loss: 0.3113\tTest loss: 0.4101\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 120\tTrain loss: 0.3197\tTest loss: 0.4128\tTest Accuracy: 30/40 (75%)\n",
      "Epoch: 121\tTrain loss: 0.3069\tTest loss: 0.3981\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 122\tTrain loss: 0.3026\tTest loss: 0.3917\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 123\tTrain loss: 0.2942\tTest loss: 0.4183\tTest Accuracy: 30/40 (75%)\n",
      "Epoch: 124\tTrain loss: 0.2985\tTest loss: 0.4008\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 125\tTrain loss: 0.2952\tTest loss: 0.3961\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 126\tTrain loss: 0.2927\tTest loss: 0.4047\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 127\tTrain loss: 0.2945\tTest loss: 0.4033\tTest Accuracy: 30/40 (75%)\n",
      "Epoch: 128\tTrain loss: 0.2934\tTest loss: 0.3925\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 129\tTrain loss: 0.2937\tTest loss: 0.3991\tTest Accuracy: 30/40 (75%)\n",
      "Epoch: 130\tTrain loss: 0.2936\tTest loss: 0.4012\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 131\tTrain loss: 0.2967\tTest loss: 0.4063\tTest Accuracy: 30/40 (75%)\n",
      "Epoch: 132\tTrain loss: 0.2951\tTest loss: 0.3925\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 133\tTrain loss: 0.2994\tTest loss: 0.3970\tTest Accuracy: 30/40 (75%)\n",
      "Epoch: 134\tTrain loss: 0.2926\tTest loss: 0.4103\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 135\tTrain loss: 0.2984\tTest loss: 0.4136\tTest Accuracy: 30/40 (75%)\n",
      "Epoch: 136\tTrain loss: 0.2982\tTest loss: 0.3919\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 137\tTrain loss: 0.2966\tTest loss: 0.3946\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 138\tTrain loss: 0.2906\tTest loss: 0.4112\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 139\tTrain loss: 0.3003\tTest loss: 0.4071\tTest Accuracy: 30/40 (75%)\n",
      "Epoch: 140\tTrain loss: 0.2984\tTest loss: 0.3861\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 141\tTrain loss: 0.3020\tTest loss: 0.3976\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 142\tTrain loss: 0.2936\tTest loss: 0.4099\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 143\tTrain loss: 0.3069\tTest loss: 0.4102\tTest Accuracy: 30/40 (75%)\n",
      "Epoch: 144\tTrain loss: 0.3058\tTest loss: 0.3880\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 145\tTrain loss: 0.3080\tTest loss: 0.3884\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 146\tTrain loss: 0.2970\tTest loss: 0.3977\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 147\tTrain loss: 0.3119\tTest loss: 0.4051\tTest Accuracy: 30/40 (75%)\n",
      "Epoch: 148\tTrain loss: 0.3004\tTest loss: 0.3920\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 149\tTrain loss: 0.2923\tTest loss: 0.3855\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 150\tTrain loss: 0.2847\tTest loss: 0.3921\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 151\tTrain loss: 0.2904\tTest loss: 0.4006\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 152\tTrain loss: 0.2891\tTest loss: 0.3807\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 153\tTrain loss: 0.2835\tTest loss: 0.3802\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 154\tTrain loss: 0.2814\tTest loss: 0.3892\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 155\tTrain loss: 0.2830\tTest loss: 0.3885\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 156\tTrain loss: 0.2854\tTest loss: 0.3810\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 157\tTrain loss: 0.2884\tTest loss: 0.3866\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 158\tTrain loss: 0.2864\tTest loss: 0.3925\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 159\tTrain loss: 0.2997\tTest loss: 0.3938\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 160\tTrain loss: 0.2977\tTest loss: 0.3848\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 161\tTrain loss: 0.3138\tTest loss: 0.3917\tTest Accuracy: 30/40 (75%)\n",
      "Epoch: 162\tTrain loss: 0.2946\tTest loss: 0.3841\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 163\tTrain loss: 0.2991\tTest loss: 0.3763\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 164\tTrain loss: 0.2834\tTest loss: 0.3805\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 165\tTrain loss: 0.2830\tTest loss: 0.3810\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 166\tTrain loss: 0.2790\tTest loss: 0.3792\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 167\tTrain loss: 0.2793\tTest loss: 0.3797\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 168\tTrain loss: 0.2781\tTest loss: 0.3774\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 169\tTrain loss: 0.2768\tTest loss: 0.3782\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 170\tTrain loss: 0.2775\tTest loss: 0.3747\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 171\tTrain loss: 0.2778\tTest loss: 0.3744\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 172\tTrain loss: 0.2768\tTest loss: 0.3861\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 173\tTrain loss: 0.2817\tTest loss: 0.3785\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 174\tTrain loss: 0.2776\tTest loss: 0.3755\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 175\tTrain loss: 0.2817\tTest loss: 0.3813\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 176\tTrain loss: 0.2821\tTest loss: 0.3773\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 177\tTrain loss: 0.2925\tTest loss: 0.3819\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 178\tTrain loss: 0.2900\tTest loss: 0.3802\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 179\tTrain loss: 0.3092\tTest loss: 0.3741\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 180\tTrain loss: 0.2841\tTest loss: 0.3800\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 181\tTrain loss: 0.2882\tTest loss: 0.3723\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 182\tTrain loss: 0.2765\tTest loss: 0.3685\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 183\tTrain loss: 0.2741\tTest loss: 0.3791\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 184\tTrain loss: 0.2735\tTest loss: 0.3720\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 185\tTrain loss: 0.2731\tTest loss: 0.3669\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 186\tTrain loss: 0.2716\tTest loss: 0.3741\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 187\tTrain loss: 0.2757\tTest loss: 0.3717\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 188\tTrain loss: 0.2735\tTest loss: 0.3739\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 189\tTrain loss: 0.2807\tTest loss: 0.3726\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 190\tTrain loss: 0.2780\tTest loss: 0.3749\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 191\tTrain loss: 0.2896\tTest loss: 0.3763\tTest Accuracy: 30/40 (75%)\n",
      "Epoch: 192\tTrain loss: 0.2853\tTest loss: 0.3709\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 193\tTrain loss: 0.3003\tTest loss: 0.3651\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 194\tTrain loss: 0.2747\tTest loss: 0.3805\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 195\tTrain loss: 0.2782\tTest loss: 0.3697\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 196\tTrain loss: 0.2687\tTest loss: 0.3661\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 197\tTrain loss: 0.2645\tTest loss: 0.3698\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 198\tTrain loss: 0.2683\tTest loss: 0.3642\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 199\tTrain loss: 0.2698\tTest loss: 0.3626\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 200\tTrain loss: 0.2669\tTest loss: 0.3702\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 201\tTrain loss: 0.2713\tTest loss: 0.3747\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 202\tTrain loss: 0.2745\tTest loss: 0.3775\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 203\tTrain loss: 0.2872\tTest loss: 0.3715\tTest Accuracy: 30/40 (75%)\n",
      "Epoch: 204\tTrain loss: 0.2812\tTest loss: 0.3750\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 205\tTrain loss: 0.3029\tTest loss: 0.3634\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 206\tTrain loss: 0.2731\tTest loss: 0.3747\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 207\tTrain loss: 0.2767\tTest loss: 0.3654\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 208\tTrain loss: 0.2649\tTest loss: 0.3645\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 209\tTrain loss: 0.2609\tTest loss: 0.3676\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 210\tTrain loss: 0.2632\tTest loss: 0.3591\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 211\tTrain loss: 0.2634\tTest loss: 0.3602\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 212\tTrain loss: 0.2623\tTest loss: 0.3582\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 213\tTrain loss: 0.2596\tTest loss: 0.3603\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 214\tTrain loss: 0.2605\tTest loss: 0.3600\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 215\tTrain loss: 0.2609\tTest loss: 0.3640\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 216\tTrain loss: 0.2618\tTest loss: 0.3611\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 217\tTrain loss: 0.2587\tTest loss: 0.3622\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 218\tTrain loss: 0.2618\tTest loss: 0.3625\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 219\tTrain loss: 0.2631\tTest loss: 0.3700\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 220\tTrain loss: 0.2712\tTest loss: 0.3648\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 221\tTrain loss: 0.2699\tTest loss: 0.3726\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 222\tTrain loss: 0.2915\tTest loss: 0.3630\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 223\tTrain loss: 0.2762\tTest loss: 0.3718\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 224\tTrain loss: 0.3012\tTest loss: 0.3532\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 225\tTrain loss: 0.2601\tTest loss: 0.3755\tTest Accuracy: 30/40 (75%)\n",
      "Epoch: 226\tTrain loss: 0.2657\tTest loss: 0.3677\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 227\tTrain loss: 0.2651\tTest loss: 0.3547\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 228\tTrain loss: 0.2544\tTest loss: 0.3691\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 229\tTrain loss: 0.2650\tTest loss: 0.3529\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 230\tTrain loss: 0.2621\tTest loss: 0.3643\tTest Accuracy: 34/40 (85%)\n",
      "Epoch: 231\tTrain loss: 0.2670\tTest loss: 0.3687\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 232\tTrain loss: 0.2690\tTest loss: 0.3663\tTest Accuracy: 34/40 (85%)\n",
      "Epoch: 233\tTrain loss: 0.2885\tTest loss: 0.3581\tTest Accuracy: 34/40 (85%)\n",
      "Epoch: 234\tTrain loss: 0.2641\tTest loss: 0.3724\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 235\tTrain loss: 0.2745\tTest loss: 0.3582\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 236\tTrain loss: 0.2598\tTest loss: 0.3572\tTest Accuracy: 34/40 (85%)\n",
      "Epoch: 237\tTrain loss: 0.2618\tTest loss: 0.3577\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 238\tTrain loss: 0.2574\tTest loss: 0.3583\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 239\tTrain loss: 0.2578\tTest loss: 0.3528\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 240\tTrain loss: 0.2574\tTest loss: 0.3613\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 241\tTrain loss: 0.2680\tTest loss: 0.3548\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 242\tTrain loss: 0.2643\tTest loss: 0.3688\tTest Accuracy: 34/40 (85%)\n",
      "Epoch: 243\tTrain loss: 0.2824\tTest loss: 0.3518\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 244\tTrain loss: 0.2555\tTest loss: 0.3642\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 245\tTrain loss: 0.2604\tTest loss: 0.3541\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 246\tTrain loss: 0.2532\tTest loss: 0.3511\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 247\tTrain loss: 0.2498\tTest loss: 0.3555\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 248\tTrain loss: 0.2533\tTest loss: 0.3531\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 249\tTrain loss: 0.2552\tTest loss: 0.3483\tTest Accuracy: 34/40 (85%)\n",
      "Epoch: 250\tTrain loss: 0.2520\tTest loss: 0.3717\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 251\tTrain loss: 0.2633\tTest loss: 0.3551\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 252\tTrain loss: 0.2572\tTest loss: 0.3677\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 253\tTrain loss: 0.2711\tTest loss: 0.3553\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 254\tTrain loss: 0.2658\tTest loss: 0.3678\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 255\tTrain loss: 0.2977\tTest loss: 0.3511\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 256\tTrain loss: 0.2521\tTest loss: 0.3742\tTest Accuracy: 30/40 (75%)\n",
      "Epoch: 257\tTrain loss: 0.2604\tTest loss: 0.3594\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 258\tTrain loss: 0.2572\tTest loss: 0.3505\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 259\tTrain loss: 0.2450\tTest loss: 0.3638\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 260\tTrain loss: 0.2565\tTest loss: 0.3412\tTest Accuracy: 34/40 (85%)\n",
      "Epoch: 261\tTrain loss: 0.2532\tTest loss: 0.3598\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 262\tTrain loss: 0.2619\tTest loss: 0.3568\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 263\tTrain loss: 0.2596\tTest loss: 0.3628\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 264\tTrain loss: 0.2757\tTest loss: 0.3467\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 265\tTrain loss: 0.2489\tTest loss: 0.3619\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 266\tTrain loss: 0.2525\tTest loss: 0.3461\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 267\tTrain loss: 0.2492\tTest loss: 0.3506\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 268\tTrain loss: 0.2513\tTest loss: 0.3517\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 269\tTrain loss: 0.2572\tTest loss: 0.3605\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 270\tTrain loss: 0.2745\tTest loss: 0.3454\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 271\tTrain loss: 0.2490\tTest loss: 0.3674\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 272\tTrain loss: 0.2576\tTest loss: 0.3482\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 273\tTrain loss: 0.2516\tTest loss: 0.3567\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 274\tTrain loss: 0.2616\tTest loss: 0.3452\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 275\tTrain loss: 0.2575\tTest loss: 0.3590\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 276\tTrain loss: 0.2743\tTest loss: 0.3444\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 277\tTrain loss: 0.2432\tTest loss: 0.3581\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 278\tTrain loss: 0.2488\tTest loss: 0.3510\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 279\tTrain loss: 0.2455\tTest loss: 0.3448\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 280\tTrain loss: 0.2403\tTest loss: 0.3446\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 281\tTrain loss: 0.2422\tTest loss: 0.3386\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 282\tTrain loss: 0.2419\tTest loss: 0.3412\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 283\tTrain loss: 0.2417\tTest loss: 0.3451\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 284\tTrain loss: 0.2409\tTest loss: 0.3451\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 285\tTrain loss: 0.2400\tTest loss: 0.3475\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 286\tTrain loss: 0.2412\tTest loss: 0.3405\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 287\tTrain loss: 0.2391\tTest loss: 0.3442\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 288\tTrain loss: 0.2404\tTest loss: 0.3392\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 289\tTrain loss: 0.2395\tTest loss: 0.3440\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 290\tTrain loss: 0.2405\tTest loss: 0.3395\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 291\tTrain loss: 0.2393\tTest loss: 0.3505\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 292\tTrain loss: 0.2419\tTest loss: 0.3394\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 293\tTrain loss: 0.2386\tTest loss: 0.3496\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 294\tTrain loss: 0.2416\tTest loss: 0.3369\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 295\tTrain loss: 0.2380\tTest loss: 0.3483\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 296\tTrain loss: 0.2406\tTest loss: 0.3406\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 297\tTrain loss: 0.2393\tTest loss: 0.3533\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 298\tTrain loss: 0.2430\tTest loss: 0.3428\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 299\tTrain loss: 0.2405\tTest loss: 0.3614\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 300\tTrain loss: 0.2542\tTest loss: 0.3455\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 301\tTrain loss: 0.2511\tTest loss: 0.3741\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 302\tTrain loss: 0.2899\tTest loss: 0.3443\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 303\tTrain loss: 0.2515\tTest loss: 0.3557\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 304\tTrain loss: 0.2601\tTest loss: 0.3374\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 305\tTrain loss: 0.2381\tTest loss: 0.3550\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 306\tTrain loss: 0.2471\tTest loss: 0.3677\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 307\tTrain loss: 0.2714\tTest loss: 0.3401\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 308\tTrain loss: 0.2380\tTest loss: 0.3670\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 309\tTrain loss: 0.2526\tTest loss: 0.3581\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 310\tTrain loss: 0.2561\tTest loss: 0.3395\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 311\tTrain loss: 0.2433\tTest loss: 0.3783\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 312\tTrain loss: 0.2913\tTest loss: 0.3362\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 313\tTrain loss: 0.2438\tTest loss: 0.3670\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 314\tTrain loss: 0.2537\tTest loss: 0.3631\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 315\tTrain loss: 0.2531\tTest loss: 0.3415\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 316\tTrain loss: 0.2342\tTest loss: 0.3556\tTest Accuracy: 34/40 (85%)\n",
      "Epoch: 317\tTrain loss: 0.2543\tTest loss: 0.3240\tTest Accuracy: 34/40 (85%)\n",
      "Epoch: 318\tTrain loss: 0.2449\tTest loss: 0.3628\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 319\tTrain loss: 0.2554\tTest loss: 0.3466\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 320\tTrain loss: 0.2490\tTest loss: 0.3585\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 321\tTrain loss: 0.2775\tTest loss: 0.3394\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 322\tTrain loss: 0.2395\tTest loss: 0.3802\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 323\tTrain loss: 0.2601\tTest loss: 0.3729\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 324\tTrain loss: 0.2726\tTest loss: 0.3668\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 325\tTrain loss: 0.2364\tTest loss: 0.3747\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 326\tTrain loss: 0.2551\tTest loss: 0.3385\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 327\tTrain loss: 0.2667\tTest loss: 0.3358\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 328\tTrain loss: 0.2351\tTest loss: 0.3806\tTest Accuracy: 30/40 (75%)\n",
      "Epoch: 329\tTrain loss: 0.2605\tTest loss: 0.3606\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 330\tTrain loss: 0.2764\tTest loss: 0.3424\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 331\tTrain loss: 0.2308\tTest loss: 0.3793\tTest Accuracy: 30/40 (75%)\n",
      "Epoch: 332\tTrain loss: 0.2583\tTest loss: 0.3505\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 333\tTrain loss: 0.2745\tTest loss: 0.3369\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 334\tTrain loss: 0.2303\tTest loss: 0.3823\tTest Accuracy: 29/40 (72%)\n",
      "Epoch: 335\tTrain loss: 0.2592\tTest loss: 0.3524\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 336\tTrain loss: 0.2776\tTest loss: 0.3377\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 337\tTrain loss: 0.2368\tTest loss: 0.3651\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 338\tTrain loss: 0.2603\tTest loss: 0.3414\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 339\tTrain loss: 0.2636\tTest loss: 0.3405\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 340\tTrain loss: 0.2283\tTest loss: 0.3721\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 341\tTrain loss: 0.2536\tTest loss: 0.3484\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 342\tTrain loss: 0.2761\tTest loss: 0.3364\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 343\tTrain loss: 0.2403\tTest loss: 0.3635\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 344\tTrain loss: 0.2612\tTest loss: 0.3528\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 345\tTrain loss: 0.2683\tTest loss: 0.3461\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 346\tTrain loss: 0.2291\tTest loss: 0.3588\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 347\tTrain loss: 0.2479\tTest loss: 0.3386\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 348\tTrain loss: 0.2597\tTest loss: 0.3323\tTest Accuracy: 34/40 (85%)\n",
      "Epoch: 349\tTrain loss: 0.2330\tTest loss: 0.3584\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 350\tTrain loss: 0.2458\tTest loss: 0.3475\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 351\tTrain loss: 0.2491\tTest loss: 0.3355\tTest Accuracy: 34/40 (85%)\n",
      "Epoch: 352\tTrain loss: 0.2284\tTest loss: 0.3474\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 353\tTrain loss: 0.2351\tTest loss: 0.3311\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 354\tTrain loss: 0.2337\tTest loss: 0.3347\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 355\tTrain loss: 0.2294\tTest loss: 0.3344\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 356\tTrain loss: 0.2298\tTest loss: 0.3452\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 357\tTrain loss: 0.2380\tTest loss: 0.3382\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 358\tTrain loss: 0.2389\tTest loss: 0.3783\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 359\tTrain loss: 0.2780\tTest loss: 0.3466\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 360\tTrain loss: 0.2356\tTest loss: 0.3623\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 361\tTrain loss: 0.2468\tTest loss: 0.3642\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 362\tTrain loss: 0.2681\tTest loss: 0.3411\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 363\tTrain loss: 0.2308\tTest loss: 0.3739\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 364\tTrain loss: 0.2521\tTest loss: 0.3544\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 365\tTrain loss: 0.2754\tTest loss: 0.3350\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 366\tTrain loss: 0.2289\tTest loss: 0.3879\tTest Accuracy: 29/40 (72%)\n",
      "Epoch: 367\tTrain loss: 0.2595\tTest loss: 0.3613\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 368\tTrain loss: 0.2858\tTest loss: 0.3398\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 369\tTrain loss: 0.2353\tTest loss: 0.3641\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 370\tTrain loss: 0.2643\tTest loss: 0.3439\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 371\tTrain loss: 0.2727\tTest loss: 0.3422\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 372\tTrain loss: 0.2287\tTest loss: 0.3586\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 373\tTrain loss: 0.2486\tTest loss: 0.3378\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 374\tTrain loss: 0.2583\tTest loss: 0.3299\tTest Accuracy: 34/40 (85%)\n",
      "Epoch: 375\tTrain loss: 0.2311\tTest loss: 0.3479\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 376\tTrain loss: 0.2421\tTest loss: 0.3455\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 377\tTrain loss: 0.2448\tTest loss: 0.3343\tTest Accuracy: 34/40 (85%)\n",
      "Epoch: 378\tTrain loss: 0.2270\tTest loss: 0.3497\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 379\tTrain loss: 0.2312\tTest loss: 0.3280\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 380\tTrain loss: 0.2288\tTest loss: 0.3355\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 381\tTrain loss: 0.2293\tTest loss: 0.3311\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 382\tTrain loss: 0.2270\tTest loss: 0.3524\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 383\tTrain loss: 0.2382\tTest loss: 0.3384\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 384\tTrain loss: 0.2376\tTest loss: 0.3750\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 385\tTrain loss: 0.2786\tTest loss: 0.3427\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 386\tTrain loss: 0.2351\tTest loss: 0.3717\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 387\tTrain loss: 0.2519\tTest loss: 0.3774\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 388\tTrain loss: 0.2824\tTest loss: 0.3451\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 389\tTrain loss: 0.2312\tTest loss: 0.3608\tTest Accuracy: 34/40 (85%)\n",
      "Epoch: 390\tTrain loss: 0.2492\tTest loss: 0.3446\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 391\tTrain loss: 0.2626\tTest loss: 0.3340\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 392\tTrain loss: 0.2282\tTest loss: 0.3641\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 393\tTrain loss: 0.2432\tTest loss: 0.3475\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 394\tTrain loss: 0.2442\tTest loss: 0.3312\tTest Accuracy: 34/40 (85%)\n",
      "Epoch: 395\tTrain loss: 0.2227\tTest loss: 0.3607\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 396\tTrain loss: 0.2345\tTest loss: 0.3258\tTest Accuracy: 34/40 (85%)\n",
      "Epoch: 397\tTrain loss: 0.2297\tTest loss: 0.3374\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 398\tTrain loss: 0.2282\tTest loss: 0.3340\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 399\tTrain loss: 0.2262\tTest loss: 0.3527\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 400\tTrain loss: 0.2359\tTest loss: 0.3321\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 401\tTrain loss: 0.2293\tTest loss: 0.3682\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 402\tTrain loss: 0.2589\tTest loss: 0.3445\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 403\tTrain loss: 0.2315\tTest loss: 0.3396\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 404\tTrain loss: 0.2297\tTest loss: 0.3333\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 405\tTrain loss: 0.2279\tTest loss: 0.3408\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 406\tTrain loss: 0.2246\tTest loss: 0.3253\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 407\tTrain loss: 0.2249\tTest loss: 0.3430\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 408\tTrain loss: 0.2407\tTest loss: 0.3229\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 409\tTrain loss: 0.2333\tTest loss: 0.3848\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 410\tTrain loss: 0.2803\tTest loss: 0.3593\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 411\tTrain loss: 0.2369\tTest loss: 0.3621\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 412\tTrain loss: 0.2489\tTest loss: 0.3810\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 413\tTrain loss: 0.2894\tTest loss: 0.3522\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 414\tTrain loss: 0.2414\tTest loss: 0.3375\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 415\tTrain loss: 0.2500\tTest loss: 0.3509\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 416\tTrain loss: 0.2521\tTest loss: 0.3334\tTest Accuracy: 34/40 (85%)\n",
      "Epoch: 417\tTrain loss: 0.2242\tTest loss: 0.3664\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 418\tTrain loss: 0.2389\tTest loss: 0.3332\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 419\tTrain loss: 0.2374\tTest loss: 0.3482\tTest Accuracy: 34/40 (85%)\n",
      "Epoch: 420\tTrain loss: 0.2571\tTest loss: 0.3577\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 421\tTrain loss: 0.2293\tTest loss: 0.3675\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 422\tTrain loss: 0.2345\tTest loss: 0.3637\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 423\tTrain loss: 0.2530\tTest loss: 0.3408\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 424\tTrain loss: 0.2253\tTest loss: 0.3471\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 425\tTrain loss: 0.2390\tTest loss: 0.3568\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 426\tTrain loss: 0.2698\tTest loss: 0.3384\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 427\tTrain loss: 0.2313\tTest loss: 0.3392\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 428\tTrain loss: 0.2455\tTest loss: 0.3452\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 429\tTrain loss: 0.2621\tTest loss: 0.3394\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 430\tTrain loss: 0.2284\tTest loss: 0.3384\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 431\tTrain loss: 0.2410\tTest loss: 0.3380\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 432\tTrain loss: 0.2445\tTest loss: 0.3280\tTest Accuracy: 34/40 (85%)\n",
      "Epoch: 433\tTrain loss: 0.2214\tTest loss: 0.3464\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 434\tTrain loss: 0.2273\tTest loss: 0.3304\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 435\tTrain loss: 0.2259\tTest loss: 0.3228\tTest Accuracy: 34/40 (85%)\n",
      "Epoch: 436\tTrain loss: 0.2196\tTest loss: 0.3271\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 437\tTrain loss: 0.2214\tTest loss: 0.3396\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 438\tTrain loss: 0.2264\tTest loss: 0.3246\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 439\tTrain loss: 0.2197\tTest loss: 0.3479\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 440\tTrain loss: 0.2264\tTest loss: 0.3230\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 441\tTrain loss: 0.2205\tTest loss: 0.3461\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 442\tTrain loss: 0.2291\tTest loss: 0.3211\tTest Accuracy: 34/40 (85%)\n",
      "Epoch: 443\tTrain loss: 0.2213\tTest loss: 0.3535\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 444\tTrain loss: 0.2413\tTest loss: 0.3232\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 445\tTrain loss: 0.2360\tTest loss: 0.3737\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 446\tTrain loss: 0.2865\tTest loss: 0.3545\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 447\tTrain loss: 0.2490\tTest loss: 0.3410\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 448\tTrain loss: 0.2527\tTest loss: 0.3557\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 449\tTrain loss: 0.2562\tTest loss: 0.3519\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 450\tTrain loss: 0.2240\tTest loss: 0.3404\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 451\tTrain loss: 0.2330\tTest loss: 0.3541\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 452\tTrain loss: 0.2704\tTest loss: 0.3305\tTest Accuracy: 34/40 (85%)\n",
      "Epoch: 453\tTrain loss: 0.2359\tTest loss: 0.3431\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 454\tTrain loss: 0.2429\tTest loss: 0.3562\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 455\tTrain loss: 0.2448\tTest loss: 0.3393\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 456\tTrain loss: 0.2186\tTest loss: 0.3467\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 457\tTrain loss: 0.2251\tTest loss: 0.3212\tTest Accuracy: 34/40 (85%)\n",
      "Epoch: 458\tTrain loss: 0.2293\tTest loss: 0.3456\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 459\tTrain loss: 0.2464\tTest loss: 0.3417\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 460\tTrain loss: 0.2263\tTest loss: 0.3358\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 461\tTrain loss: 0.2245\tTest loss: 0.3309\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 462\tTrain loss: 0.2224\tTest loss: 0.3391\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 463\tTrain loss: 0.2193\tTest loss: 0.3215\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 464\tTrain loss: 0.2189\tTest loss: 0.3330\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 465\tTrain loss: 0.2275\tTest loss: 0.3107\tTest Accuracy: 34/40 (85%)\n",
      "Epoch: 466\tTrain loss: 0.2186\tTest loss: 0.3562\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 467\tTrain loss: 0.2474\tTest loss: 0.3441\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 468\tTrain loss: 0.2287\tTest loss: 0.3308\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 469\tTrain loss: 0.2221\tTest loss: 0.3316\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 470\tTrain loss: 0.2213\tTest loss: 0.3312\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 471\tTrain loss: 0.2178\tTest loss: 0.3189\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 472\tTrain loss: 0.2189\tTest loss: 0.3291\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 473\tTrain loss: 0.2240\tTest loss: 0.3126\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 474\tTrain loss: 0.2164\tTest loss: 0.3486\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 475\tTrain loss: 0.2270\tTest loss: 0.3248\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 476\tTrain loss: 0.2182\tTest loss: 0.3486\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 477\tTrain loss: 0.2323\tTest loss: 0.3215\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 478\tTrain loss: 0.2302\tTest loss: 0.3716\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 479\tTrain loss: 0.2824\tTest loss: 0.3592\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 480\tTrain loss: 0.2504\tTest loss: 0.3414\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 481\tTrain loss: 0.2485\tTest loss: 0.3477\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 482\tTrain loss: 0.2447\tTest loss: 0.3488\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 483\tTrain loss: 0.2194\tTest loss: 0.3353\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 484\tTrain loss: 0.2201\tTest loss: 0.3475\tTest Accuracy: 34/40 (85%)\n",
      "Epoch: 485\tTrain loss: 0.2339\tTest loss: 0.3052\tTest Accuracy: 34/40 (85%)\n",
      "Epoch: 486\tTrain loss: 0.2212\tTest loss: 0.3676\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 487\tTrain loss: 0.2340\tTest loss: 0.3489\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 488\tTrain loss: 0.2235\tTest loss: 0.3629\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 489\tTrain loss: 0.2329\tTest loss: 0.3277\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 490\tTrain loss: 0.2307\tTest loss: 0.3727\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 491\tTrain loss: 0.2873\tTest loss: 0.3683\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 492\tTrain loss: 0.2536\tTest loss: 0.3416\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 493\tTrain loss: 0.2469\tTest loss: 0.3531\tTest Accuracy: 31/40 (78%)\n",
      "Epoch: 494\tTrain loss: 0.2489\tTest loss: 0.3771\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 495\tTrain loss: 0.2566\tTest loss: 0.3814\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 496\tTrain loss: 0.2413\tTest loss: 0.3462\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 497\tTrain loss: 0.2458\tTest loss: 0.3200\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 498\tTrain loss: 0.2530\tTest loss: 0.3357\tTest Accuracy: 33/40 (82%)\n",
      "Epoch: 499\tTrain loss: 0.2307\tTest loss: 0.3492\tTest Accuracy: 32/40 (80%)\n",
      "Epoch: 500\tTrain loss: 0.2361\tTest loss: 0.3405\tTest Accuracy: 32/40 (80%)\n"
     ]
    }
   ],
   "source": [
    "# number of iterations\n",
    "epochs = 500\n",
    "# step width in gradient-based optimization\n",
    "learning_rate = 0.01\n",
    "# initialize our model with random parameters\n",
    "model = Net()\n",
    "# choose an optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "# choose a loss function\n",
    "loss_function = F.nll_loss\n",
    "\n",
    "# initialize lists for plotting\n",
    "learning_curve_train = []\n",
    "learning_curve_test = []\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    \n",
    "    # temporary variables to track model convergence\n",
    "    loss_train = []\n",
    "    loss_test = []\n",
    "    correct = 0\n",
    "    \n",
    "    # Training Loop\n",
    "    for data, target in train_dataloader:\n",
    "        # set gradient to zero\n",
    "        optimizer.zero_grad()\n",
    "        # predict with current model\n",
    "        output = model(data)\n",
    "        # compute loss of predictions and target values\n",
    "        loss = loss_function(output, target)\n",
    "        # Backpropagation of errors gives gradients\n",
    "        loss.backward()\n",
    "        # Update weights with approximate gradient\n",
    "        optimizer.step()\n",
    "        # Store loss\n",
    "        loss_train.append(loss.item())\n",
    "\n",
    "    # Stop tracking gradients for evaluation phase\n",
    "    with torch.no_grad():\n",
    "        # Test Loop\n",
    "        for data, target in test_dataloader:\n",
    "            # predict with current model\n",
    "            output = model(data)\n",
    "            # compute loss\n",
    "            loss = loss_function(output, target)\n",
    "            # store loss\n",
    "            loss_test.append(loss.item())\n",
    "            # transform output into predicted class\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            # compare predictions and labels and store \n",
    "            # number of correct predictions\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    # Store loss at each iteration for plotting\n",
    "    learning_curve_train.append(np.mean(loss_train))\n",
    "    learning_curve_test.append(np.mean(loss_test))\n",
    "    \n",
    "    # Print to Stdout\n",
    "    print('Epoch: {}\\tTrain loss: {:.4f}\\tTest loss: {:.4f}\\tTest Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "        epoch, learning_curve_train[-1], learning_curve_test[-1], \n",
    "        correct, len(test_dataloader.dataset),\n",
    "        100. * correct / len(test_dataloader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the learning curves of training and test data by plotting the loss at each epoch against the number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(epochs),learning_curve_train, label='Train')\n",
    "plt.plot(range(epochs),learning_curve_test,label='Test', color = \"g\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Test Loss')\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With your final model, predict the classes of the training and testing set one last time and visualize the corresponding confusion matrices. To do so, remember that the output of your model consists of a likelihood\n",
    "for each class. Use the `argmax()` function to get the predicted class\n",
    "label. To transform your torch tensors to python data you can use the\n",
    "`.item()`-method. You can then use `sklearn.metrics.confusion_matrix()` to obtain\n",
    "the confusion matrices, and `plt.imshow` to visualize them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(dataset_train, batch_size=1) #Batch size should be 1, if it is e.g. 64, then we have a tensor with 64 elements.\n",
    "test_dataloader = DataLoader(dataset_test, batch_size=1)\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(1,2,1)\n",
    "true = []\n",
    "pred = []\n",
    "for data, target in train_dataloader:\n",
    "    output = model(data)\n",
    "    true.append(target.item())\n",
    "    pred.append(torch.argmax(output).item())\n",
    "\n",
    "plt.imshow(confusion_matrix(true,pred))\n",
    "plt.colorbar()\n",
    "plt.title(\"Train\")\n",
    "\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "true2 = []\n",
    "pred2 = []\n",
    "for data, target in test_dataloader:\n",
    "    output = model(data)\n",
    "    true2.append(target.item())\n",
    "    pred2.append(torch.argmax(output).item())\n",
    "\n",
    "plt.imshow(confusion_matrix(true2,pred2))\n",
    "plt.colorbar()\n",
    "plt.title(\"Test\")\n",
    "plt.show()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The change of color to yellow on (also around) the diagonal means that the prediction is getting better. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
